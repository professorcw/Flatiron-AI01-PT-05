{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0c604cd",
   "metadata": {},
   "source": [
    "Step 1: Problem Definition and Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42ea7ec1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pos_tag\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mapi\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808ecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Support Ticket Data\n",
    "df = pd.read_csv('customer_support_ticket.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb934612",
   "metadata": {},
   "source": [
    "Step 2: Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832eb4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Lemmatizer relies of part of speech to help\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "  '''\n",
    "  Translate nltk POS to wordnet tags\n",
    "  '''\n",
    "  if treebank_tag.startswith('J'):\n",
    "      return wordnet.ADJ\n",
    "  elif treebank_tag.startswith('V'):\n",
    "      return wordnet.VERB\n",
    "  elif treebank_tag.startswith('N'):\n",
    "      return wordnet.NOUN\n",
    "  elif treebank_tag.startswith('R'):\n",
    "      return wordnet.ADV\n",
    "  else:\n",
    "      return wordnet.NOUN\n",
    "\n",
    "\n",
    "def basic_preprocess(text):\n",
    "   \"\"\"Basic preprocessing function for text.\"\"\"\n",
    "   # Convert to lowercase\n",
    "   text = text.lower()\n",
    "  \n",
    "   # Remove special characters and numbers\n",
    "   text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "  \n",
    "   # Return cleaned text\n",
    "   return text\n",
    "\n",
    "\n",
    "def advanced_preprocess(text):\n",
    "   \"\"\"Advanced preprocessing with tokenization, stopword removal, and lemmatization.\"\"\"\n",
    "   # Basic cleaning\n",
    "   text = basic_preprocess(text)\n",
    "  \n",
    "   # Tokenize\n",
    "   tokens = nltk.word_tokenize(text)\n",
    "  \n",
    "   # Tag with pos\n",
    "   tokens_tagged = pos_tag(tokens)\n",
    "   pos_tokens = [(word[0], get_wordnet_pos(word[1])) for word in tokens_tagged]\n",
    "  \n",
    "   # Remove stopwords and lemmatize\n",
    "   cleaned_tokens = [lemmatizer.lemmatize(token[0], token[1]) for token in pos_tokens if token[0] not in stop_words and len(token[0]) > 1]\n",
    "  \n",
    "   # Return cleaned tokens\n",
    "   return ' '.join(cleaned_tokens)\n",
    "\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "df['cleaned_text'] = df['ticket_text'].apply(basic_preprocess)\n",
    "df['lemmatized_text'] = df['ticket_text'].apply(advanced_preprocess)\n",
    "\n",
    "\n",
    "# Show the preprocessing results for a sample ticket\n",
    "sample_idx = 1\n",
    "print(f\"Original: {df.loc[sample_idx, 'ticket_text']}\")\n",
    "print(f\"Cleaned: {df.loc[sample_idx, 'cleaned_text']}\")\n",
    "print(f\"Lemmatized: {df.loc[sample_idx, 'lemmatized_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19b91cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "   df['ticket_text'],\n",
    "   df['department'],\n",
    "   test_size=0.3,\n",
    "   random_state=42,\n",
    "   stratify=df['department']  # Ensure balanced classes in both sets\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "print(f\"Class distribution in training set: \\n{y_train.value_counts()}\")\n",
    "print(f\"Class distribution in testing set: \\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e467f94c",
   "metadata": {},
   "source": [
    "Step 3: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4de23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different text vectorizers to convert text to numerical features\n",
    "\n",
    "# Bag of Words vectorizer\n",
    "count_vectorizer = CountVectorizer(\n",
    "   preprocessor=advanced_preprocess,\n",
    "   lowercase=False,  # Already done in preprocessing\n",
    "   min_df=2,  # Ignore terms that appear in fewer than 2 documents\n",
    "   max_df=0.95, # Ignore terms that appear in more than 95% of documents\n",
    "   ngram_range=(1, 2) # Include both single words and pairs of consecutive words\n",
    ")\n",
    "\n",
    "\n",
    "# TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "   preprocessor=advanced_preprocess,\n",
    "   lowercase=False,\n",
    "   min_df=2,\n",
    "   max_df=0.95,\n",
    "   ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "\n",
    "# Apply vectorizers to training data\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "# Get feature information\n",
    "count_features = count_vectorizer.get_feature_names_out()\n",
    "tfidf_features = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "print(f\"Bag of Words features: {len(count_features)}\")\n",
    "print(f\"TF-IDF features: {len(tfidf_features)}\")\n",
    "print(f\"Sample BoW features: {count_features[:10]}\")\n",
    "print(f\"Sample TF-IDF features (including bigrams): {[f for f in tfidf_features[:20] if ' ' in f][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be85e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Word2Vec embeddings for advanced feature representation\n",
    "\n",
    "# Train a Word2Vec model on our dataset\n",
    "# Note: In a real-world scenario, you'd use a much larger corpus\n",
    "# or pre-trained embeddings for better results\n",
    "w2v_model = Word2Vec(\n",
    "   df['tokens'],\n",
    "   vector_size=100,  # Dimension of the embedding vectors\n",
    "   window=5,  # Context window size\n",
    "   min_count=1,  # Ignore words with fewer occurrences\n",
    "   workers=4,  # Number of processors to use\n",
    "   sg=1  # Skip-gram model (1) instead of CBOW (0)\n",
    ")\n",
    "\n",
    "\n",
    "# Function to create document vectors by averaging word vectors\n",
    "def document_to_vector(tokens, model, vector_size=100):\n",
    "   \"\"\"Convert a document (list of tokens) to a vector using word embeddings.\"\"\"\n",
    "   # Initialize an empty vector\n",
    "   doc_vector = np.zeros(vector_size)\n",
    "  \n",
    "   # Count valid tokens\n",
    "   valid_token_count = 0\n",
    "  \n",
    "   # Sum up vectors for each token\n",
    "   for token in tokens:\n",
    "       if token in model.wv:\n",
    "           doc_vector += model.wv[token]\n",
    "           valid_token_count += 1\n",
    "  \n",
    "   # Average the vectors\n",
    "   if valid_token_count > 0:\n",
    "       doc_vector /= valid_token_count\n",
    "      \n",
    "   return doc_vector\n",
    "\n",
    "\n",
    "def document_to_vector_pretrained(tokens, model, vector_size=300):\n",
    "   \"\"\"Convert a document (list of tokens) to a vector using word embeddings.\"\"\"\n",
    "   # Initialize an empty vector\n",
    "   doc_vector = np.zeros(vector_size)\n",
    "  \n",
    "   # Count valid tokens\n",
    "   valid_token_count = 0\n",
    "  \n",
    "   # Sum up vectors for each token\n",
    "   for token in tokens:\n",
    "       if token in model:\n",
    "           doc_vector += model[token]\n",
    "           valid_token_count += 1\n",
    "  \n",
    "   # Average the vectors\n",
    "   if valid_token_count > 0:\n",
    "       doc_vector /= valid_token_count\n",
    "      \n",
    "   return doc_vector\n",
    "\n",
    "\n",
    "# Create document vectors for training and test sets\n",
    "X_train_w2v = np.array([document_to_vector(tokens, w2v_model) for tokens in X_train_tokens])\n",
    "X_test_w2v = np.array([document_to_vector(tokens, w2v_model) for tokens in X_test_tokens])\n",
    "\n",
    "\n",
    "print(f\"Word2Vec document vectors shape: {X_train_w2v.shape}\")\n",
    "\n",
    "\n",
    "# Alternatively, download and use pre-trained embeddings\n",
    "# This takes more time but might give better results\n",
    "try:\n",
    "   # Attempt to download pre-trained embeddings (if internet is available)\n",
    "   pretrained_model = api.load('word2vec-google-news-300')\n",
    "   print(\"Pre-trained model loaded successfully.\")\n",
    "  \n",
    "   # Create vectors using pre-trained embeddings\n",
    "   X_train_pretrained = np.array([document_to_vector_pretrained(tokens, pretrained_model, 300)\n",
    "                                   for tokens in X_train_tokens])\n",
    "   X_test_pretrained = np.array([document_to_vector_pretrained(tokens, pretrained_model, 300)\n",
    "                                  for tokens in X_test_tokens])\n",
    "  \n",
    "   print(f\"Pre-trained document vectors shape: {X_train_pretrained.shape}\")\n",
    "   pretrained_available = True\n",
    "except Exception as e:\n",
    "   print(f\"Pre-trained embeddings could not be loaded: {e}\")\n",
    "   pretrained_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b06edef",
   "metadata": {},
   "source": [
    "Step 4: Model Selectxion and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10139389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes models with different feature representations\n",
    "\n",
    "# Train with Bag of Words features\n",
    "nb_bow = MultinomialNB(alpha=1.0)\n",
    "nb_bow.fit(X_train_counts, y_train)\n",
    "\n",
    "\n",
    "# Train with TF-IDF features\n",
    "nb_tfidf = MultinomialNB(alpha=1.0)\n",
    "nb_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "\n",
    "# For Word2Vec features, we need to convert the data to a non-negative representation\n",
    "# because Multinomial NB requires non-negative feature values\n",
    "# One simple approach is min-max scaling\n",
    "def min_max_scale(X):\n",
    "   \"\"\"Scale features to [0, 1] range.\"\"\"\n",
    "   X_min = X.min(axis=0)\n",
    "   X_max = X.max(axis=0)\n",
    "   return (X - X_min) / (X_max - X_min + 1e-10)  # Adding a small epsilon to avoid division by zero\n",
    "\n",
    "\n",
    "X_train_w2v_scaled = min_max_scale(X_train_w2v)\n",
    "X_test_w2v_scaled = min_max_scale(X_test_w2v)\n",
    "\n",
    "\n",
    "# Train with Word2Vec features\n",
    "nb_w2v = MultinomialNB(alpha=1.0)\n",
    "nb_w2v.fit(X_train_w2v_scaled, y_train)\n",
    "\n",
    "\n",
    "# Also train with pre-trained embeddings if available\n",
    "if pretrained_available:\n",
    "   X_train_pretrained_scaled = min_max_scale(X_train_pretrained)\n",
    "   X_test_pretrained_scaled = min_max_scale(X_test_pretrained)\n",
    "  \n",
    "   nb_pretrained = MultinomialNB(alpha=1.0)\n",
    "   nb_pretrained.fit(X_train_pretrained_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cbfba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display top features by class\n",
    "def display_top_features(classifier, vectorizer, class_labels, n=10):\n",
    "   \"\"\"Display the top n words for each class based on their likelihood.\"\"\"\n",
    "   feature_names = vectorizer.get_feature_names_out()\n",
    "   for i, class_label in enumerate(class_labels):\n",
    "       top_indices = np.argsort(classifier.feature_log_prob_[i])[-n:][::-1]\n",
    "       top_features = [feature_names[j] for j in top_indices]\n",
    "       print(f\"\\nTop words for '{class_label}':\")\n",
    "       print(\", \".join(top_features))\n",
    "\n",
    "\n",
    "# Display top features for Bag of Words model\n",
    "print(\"\\nTop discriminative features per department (Bag of Words model):\")\n",
    "display_top_features(nb_bow, count_vectorizer, nb_bow.classes_)\n",
    "\n",
    "\n",
    "# Display top features for BoW model, focusing on bigrams\n",
    "print(\"\\nTop discriminative bigrams per department (Bag of Words model):\")\n",
    "def display_top_ngrams(classifier, vectorizer, class_labels, n=5):\n",
    "   \"\"\"Display the top n bigrams for each class based on their likelihood.\"\"\"\n",
    "   feature_names = vectorizer.get_feature_names_out()\n",
    "   # Get indices of all bigram features\n",
    "   bigram_indices = [i for i, feat in enumerate(feature_names) if ' ' in feat]\n",
    "  \n",
    "   for i, class_label in enumerate(class_labels):\n",
    "       # Filter to only consider bigram features\n",
    "       bigram_log_probs = [(j, classifier.feature_log_prob_[i][j]) for j in bigram_indices]\n",
    "       # Sort by probability (highest first)\n",
    "       top_bigram_indices = sorted(bigram_log_probs, key=lambda x: x[1], reverse=True)[:n]\n",
    "       top_bigrams = [feature_names[j] for j, _ in top_bigram_indices]\n",
    "       print(f\"\\nTop bigrams for '{class_label}':\")\n",
    "       print(\", \".join(top_bigrams))\n",
    "\n",
    "\n",
    "display_top_bigrams(nb_bow, count_vectorizer, nb_bow.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9443d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word embeddings to understand semantic relationships\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Function to visualize word embeddings in 2D\n",
    "def visualize_embeddings(model, words=None, n=50):\n",
    "   \"\"\"Visualize word embeddings in a 2D space.\"\"\"\n",
    "   # If no specific words provided, use most frequent words\n",
    "   if words is None:\n",
    "       # Get word counts from the corpus\n",
    "       from collections import Counter\n",
    "       all_words = [word for doc in df['tokenized_text'] for word in doc]\n",
    "       word_counts = Counter(all_words)\n",
    "       words = [word for word, count in word_counts.most_common(n)]\n",
    "  \n",
    "   # Filter for words in the model's vocabulary\n",
    "   valid_words = [word for word in words if word in model]\n",
    "  \n",
    "   if len(valid_words) == 0:\n",
    "       print(\"No valid words found in model vocabulary!\")\n",
    "       return\n",
    "  \n",
    "   # Get word vectors\n",
    "   word_vectors = [model[word] for word in valid_words]\n",
    "  \n",
    "   # Reduce to 2 dimensions with PCA\n",
    "   pca = PCA(n_components=2)\n",
    "   reduced_vectors = pca.fit_transform(word_vectors)\n",
    "  \n",
    "   # Plot the words in 2D space\n",
    "   plt.figure(figsize=(12, 10))\n",
    "   plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], s=10)\n",
    "  \n",
    "   # Add word labels\n",
    "   for i, word in enumerate(valid_words):\n",
    "       plt.annotate(word, (reduced_vectors[i, 0], reduced_vectors[i, 1]),\n",
    "                   fontsize=9, alpha=0.7)\n",
    "  \n",
    "   plt.title('Word Embeddings Visualization')\n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "\n",
    "\n",
    "# Visualize embeddings for department-specific words\n",
    "department_words = {\n",
    "   'technical': ['login', 'password', 'error', 'crash', 'app', 'loading', 'website'],\n",
    "   'account': ['update', 'change', 'information', 'address', 'settings', 'preferences'],\n",
    "   'billing': ['payment', 'charge', 'refund', 'credit', 'invoice', 'subscription']\n",
    "}\n",
    "\n",
    "\n",
    "# Flatten the word list\n",
    "all_dept_words = [word for words in department_words.values() for word in words]\n",
    "\n",
    "\n",
    "# Visualize the embeddings\n",
    "print(\"Visualizing word embeddings for department-specific terms:\")\n",
    "visualize_embeddings(pretrained_model, all_dept_words)\n",
    "\n",
    "\n",
    "# Also visualize some general support terms to see relationships\n",
    "general_terms = ['help', 'issue', 'problem', 'support', 'request', 'question',\n",
    "               'account', 'order', 'customer', 'service']\n",
    "visualize_embeddings(pretrained_model, general_terms + all_dept_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05dcb19",
   "metadata": {},
   "source": [
    "Step 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ce83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data for each feature representation and make predictions\n",
    "\n",
    "# Transform test data using the fitted vectorizers\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "# Word2Vec test data was already prepared above\n",
    "\n",
    "\n",
    "# Predict with Bag of Words model\n",
    "y_pred_bow = nb_bow.predict(X_test_counts)\n",
    "\n",
    "\n",
    "# Predict with TF-IDF model\n",
    "y_pred_tfidf = nb_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "\n",
    "# Predict with Word2Vec model\n",
    "y_pred_w2v = nb_w2v.predict(X_test_w2v_scaled)\n",
    "\n",
    "\n",
    "# Predict with pre-trained embeddings if available\n",
    "y_pred_pretrained = nb_pretrained.predict(X_test_pretrained_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fb20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and compare model performance\n",
    "\n",
    "# Function to evaluate and display model performance\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "   \"\"\"Evaluate a model and print performance metrics.\"\"\"\n",
    "   accuracy = accuracy_score(y_true, y_pred)\n",
    "   print(f\"\\n{model_name} Model Performance:\")\n",
    "   print(f\"Accuracy: {accuracy:.4f}\")\n",
    "   print(\"\\nClassification Report:\")\n",
    "   print(classification_report(y_true, y_pred))\n",
    "  \n",
    "   # Create a confusion matrix\n",
    "   cm = confusion_matrix(y_true, y_pred)\n",
    "  \n",
    "   # Plot confusion matrix as a heatmap\n",
    "   plt.figure(figsize=(8, 6))\n",
    "   sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "              xticklabels=sorted(set(y_true)),\n",
    "              yticklabels=sorted(set(y_true)))\n",
    "   plt.title(f'Confusion Matrix - {model_name}')\n",
    "   plt.xlabel('Predicted')\n",
    "   plt.ylabel('True')\n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "  \n",
    "   return accuracy\n",
    "\n",
    "\n",
    "# Evaluate all models\n",
    "bow_accuracy = evaluate_model(y_test, y_pred_bow, \"Bag of Words\")\n",
    "tfidf_accuracy = evaluate_model(y_test, y_pred_tfidf, \"TF-IDF\")\n",
    "w2v_accuracy = evaluate_model(y_test, y_pred_w2v, \"Word2Vec\")\n",
    "\n",
    "\n",
    "if pretrained_available:\n",
    "   pretrained_accuracy = evaluate_model(y_test, y_pred_pretrained, \"Pre-trained Embeddings\")\n",
    "\n",
    "\n",
    "# Compare models\n",
    "models = [\"Bag of Words\", \"TF-IDF\", \"Word2Vec\"]\n",
    "accuracies = [bow_accuracy, tfidf_accuracy, w2v_accuracy]\n",
    "\n",
    "\n",
    "if pretrained_available:\n",
    "   models.append(\"Pre-trained Embeddings\")\n",
    "   accuracies.append(pretrained_accuracy)\n",
    "\n",
    "\n",
    "# Plot model comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=models, y=accuracies)\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "for i, acc in enumerate(accuracies):\n",
    "   plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e6d92a",
   "metadata": {},
   "source": [
    "Step 6: Model Refinement and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eaf605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with hyperparameter tuning to improve model performance. We will go ahead and attempt to tune the TF-IDF vectorizer and associated mode\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Create a pipeline for the TF-IDF approach\n",
    "tfidf_pipeline = Pipeline([\n",
    "   ('vectorizer', TfidfVectorizer(preprocessor=advanced_preprocess, lowercase=False)),\n",
    "   ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "\n",
    "# Define parameter grid to search\n",
    "param_grid = {\n",
    "   'vectorizer__min_df': [1, 2, 3],\n",
    "   'vectorizer__max_df': [0.9, 0.95, 1.0],\n",
    "   'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "   'classifier__alpha': [0.1, 0.5, 1.0, 2.0]\n",
    "}\n",
    "\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "   tfidf_pipeline,\n",
    "   param_grid,\n",
    "   cv=5,             # 5-fold cross-validation\n",
    "   scoring='accuracy',\n",
    "   verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the grid search to the data\n",
    "print(\"Performing grid search to optimize model hyperparameters...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Print best parameters and score\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "\n",
    "# Evaluate the optimized model on test data\n",
    "optimized_pipeline = grid_search.best_estimator_\n",
    "y_pred_optimized = optimized_pipeline.predict(X_test)\n",
    "optimized_accuracy = evaluate_model(y_test, y_pred_optimized, \"Optimized TF-IDF\")\n",
    "\n",
    "\n",
    "# Add to our model comparison\n",
    "models.append(\"Optimized TF-IDF\")\n",
    "accuracies.append(optimized_accuracy)\n",
    "\n",
    "\n",
    "# Updated plot for model comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=models, y=accuracies)\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "for i, acc in enumerate(accuracies):\n",
    "   plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai-env)",
   "language": "python",
   "name": "ai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
