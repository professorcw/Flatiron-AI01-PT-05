{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0c604cd",
   "metadata": {},
   "source": [
    "Step 1: Problem Definition and Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ea7ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3808ecbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket_id</th>\n",
       "      <th>ticket_text</th>\n",
       "      <th>department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Cannot login to my account after password reset</td>\n",
       "      <td>technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The website is loading very slowly on my browser</td>\n",
       "      <td>technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The app crashes whenever I try to upload photos</td>\n",
       "      <td>technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Videos are not playing properly on my device</td>\n",
       "      <td>technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The checkout process gave me an error</td>\n",
       "      <td>technical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticket_id                                       ticket_text department\n",
       "0          1   Cannot login to my account after password reset  technical\n",
       "1          2  The website is loading very slowly on my browser  technical\n",
       "2          3   The app crashes whenever I try to upload photos  technical\n",
       "3          4      Videos are not playing properly on my device  technical\n",
       "4          5             The checkout process gave me an error  technical"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in Support Ticket Data\n",
    "df = pd.read_csv('customer_support_ticket.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb934612",
   "metadata": {},
   "source": [
    "Step 2: Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "832eb4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\craig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\craig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\craig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The website is loading very slowly on my browser\n",
      "Cleaned: the website is loading very slowly on my browser\n",
      "Lemmatized: website load slowly browser\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Lemmatizer relies of part of speech to help\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "  '''\n",
    "  Translate nltk POS to wordnet tags\n",
    "  '''\n",
    "  if treebank_tag.startswith('J'):\n",
    "      return wordnet.ADJ\n",
    "  elif treebank_tag.startswith('V'):\n",
    "      return wordnet.VERB\n",
    "  elif treebank_tag.startswith('N'):\n",
    "      return wordnet.NOUN\n",
    "  elif treebank_tag.startswith('R'):\n",
    "      return wordnet.ADV\n",
    "  else:\n",
    "      return wordnet.NOUN\n",
    "\n",
    "\n",
    "def basic_preprocess(text):\n",
    "   \"\"\"Basic preprocessing function for text.\"\"\"\n",
    "   # Convert to lowercase\n",
    "   text = text.lower()\n",
    "  \n",
    "   # Remove special characters and numbers\n",
    "   text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "  \n",
    "   # Return cleaned text\n",
    "   return text\n",
    "\n",
    "\n",
    "def advanced_preprocess(text):\n",
    "   \"\"\"Advanced preprocessing with tokenization, stopword removal, and lemmatization.\"\"\"\n",
    "   # Basic cleaning\n",
    "   text = basic_preprocess(text)\n",
    "  \n",
    "   # Tokenize\n",
    "   tokens = nltk.word_tokenize(text)\n",
    "  \n",
    "   # Tag with pos\n",
    "   tokens_tagged = pos_tag(tokens)\n",
    "   pos_tokens = [(word[0], get_wordnet_pos(word[1])) for word in tokens_tagged]\n",
    "  \n",
    "   # Remove stopwords and lemmatize\n",
    "   cleaned_tokens = [lemmatizer.lemmatize(token[0], token[1]) for token in pos_tokens if token[0] not in stop_words and len(token[0]) > 1]\n",
    "  \n",
    "   # Return cleaned tokens\n",
    "   return ' '.join(cleaned_tokens)\n",
    "\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "df['cleaned_text'] = df['ticket_text'].apply(basic_preprocess)\n",
    "df['lemmatized_text'] = df['ticket_text'].apply(advanced_preprocess)\n",
    "\n",
    "\n",
    "# Show the preprocessing results for a sample ticket\n",
    "sample_idx = 1\n",
    "print(f\"Original: {df.loc[sample_idx, 'ticket_text']}\")\n",
    "print(f\"Cleaned: {df.loc[sample_idx, 'cleaned_text']}\")\n",
    "print(f\"Lemmatized: {df.loc[sample_idx, 'lemmatized_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d19b91cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 63\n",
      "Testing set size: 27\n",
      "Class distribution in training set: \n",
      "department\n",
      "technical    21\n",
      "account      21\n",
      "billing      21\n",
      "Name: count, dtype: int64\n",
      "Class distribution in testing set: \n",
      "department\n",
      "account      9\n",
      "technical    9\n",
      "billing      9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "   df['ticket_text'],\n",
    "   df['department'],\n",
    "   test_size=0.3,\n",
    "   random_state=42,\n",
    "   stratify=df['department']  # Ensure balanced classes in both sets\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "print(f\"Class distribution in training set: \\n{y_train.value_counts()}\")\n",
    "print(f\"Class distribution in testing set: \\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e467f94c",
   "metadata": {},
   "source": [
    "Step 3: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd4de23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words features: 57\n",
      "TF-IDF features: 57\n",
      "Sample BoW features: ['access' 'account' 'account password' 'account show' 'add' 'address'\n",
      " 'app' 'billing' 'browser' 'change']\n",
      "Sample TF-IDF features (including bigrams): ['account password', 'account show', 'get error']\n"
     ]
    }
   ],
   "source": [
    "# Create different text vectorizers to convert text to numerical features\n",
    "\n",
    "# Bag of Words vectorizer\n",
    "count_vectorizer = CountVectorizer(\n",
    "   preprocessor=advanced_preprocess,\n",
    "   lowercase=False,  # Already done in preprocessing\n",
    "   min_df=2,  # Ignore terms that appear in fewer than 2 documents\n",
    "   max_df=0.95, # Ignore terms that appear in more than 95% of documents\n",
    "   ngram_range=(1, 2) # Include both single words and pairs of consecutive words\n",
    ")\n",
    "\n",
    "\n",
    "# TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "   preprocessor=advanced_preprocess,\n",
    "   lowercase=False,\n",
    "   min_df=2,\n",
    "   max_df=0.95,\n",
    "   ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "\n",
    "# Apply vectorizers to training data\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "# Get feature information\n",
    "count_features = count_vectorizer.get_feature_names_out()\n",
    "tfidf_features = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "print(f\"Bag of Words features: {len(count_features)}\")\n",
    "print(f\"TF-IDF features: {len(tfidf_features)}\")\n",
    "print(f\"Sample BoW features: {count_features[:10]}\")\n",
    "print(f\"Sample TF-IDF features (including bigrams): {[f for f in tfidf_features[:20] if ' ' in f][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "685dbe83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket_id</th>\n",
       "      <th>ticket_text</th>\n",
       "      <th>department</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Cannot login to my account after password reset</td>\n",
       "      <td>technical</td>\n",
       "      <td>cannot login to my account after password reset</td>\n",
       "      <td>login account password reset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The website is loading very slowly on my browser</td>\n",
       "      <td>technical</td>\n",
       "      <td>the website is loading very slowly on my browser</td>\n",
       "      <td>website load slowly browser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The app crashes whenever I try to upload photos</td>\n",
       "      <td>technical</td>\n",
       "      <td>the app crashes whenever i try to upload photos</td>\n",
       "      <td>app crash whenever try upload photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Videos are not playing properly on my device</td>\n",
       "      <td>technical</td>\n",
       "      <td>videos are not playing properly on my device</td>\n",
       "      <td>video play properly device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The checkout process gave me an error</td>\n",
       "      <td>technical</td>\n",
       "      <td>the checkout process gave me an error</td>\n",
       "      <td>checkout process give error</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticket_id                                       ticket_text department  \\\n",
       "0          1   Cannot login to my account after password reset  technical   \n",
       "1          2  The website is loading very slowly on my browser  technical   \n",
       "2          3   The app crashes whenever I try to upload photos  technical   \n",
       "3          4      Videos are not playing properly on my device  technical   \n",
       "4          5             The checkout process gave me an error  technical   \n",
       "\n",
       "                                       cleaned_text  \\\n",
       "0   cannot login to my account after password reset   \n",
       "1  the website is loading very slowly on my browser   \n",
       "2   the app crashes whenever i try to upload photos   \n",
       "3      videos are not playing properly on my device   \n",
       "4             the checkout process gave me an error   \n",
       "\n",
       "                       lemmatized_text  \n",
       "0         login account password reset  \n",
       "1          website load slowly browser  \n",
       "2  app crash whenever try upload photo  \n",
       "3           video play properly device  \n",
       "4          checkout process give error  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be85e793",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 63\u001b[0m\n\u001b[0;32m     59\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m doc_vector\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Create document vectors for training and test sets\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m X_train_w2v \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([document_to_vector(tokens, w2v_model) \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m \u001b[43mX_train_tokens\u001b[49m])\n\u001b[0;32m     64\u001b[0m X_test_w2v \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([document_to_vector(tokens, w2v_model) \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m X_test_tokens])\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord2Vec document vectors shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_w2v\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "# Create Word2Vec embeddings for advanced feature representation\n",
    "\n",
    "# Train a Word2Vec model on our dataset\n",
    "# Note: In a real-world scenario, you'd use a much larger corpus\n",
    "# or pre-trained embeddings for better results\n",
    "\n",
    "df['tokens'] = df['lemmatized_text'].apply(lambda x: x.split())\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "   df['tokens'],\n",
    "   vector_size=100,  # Dimension of the embedding vectors\n",
    "   window=5,  # Context window size\n",
    "   min_count=1,  # Ignore words with fewer occurrences\n",
    "   workers=4,  # Number of processors to use\n",
    "   sg=1  # Skip-gram model (1) instead of CBOW (0)\n",
    ")\n",
    "\n",
    "\n",
    "# Function to create document vectors by averaging word vectors\n",
    "def document_to_vector(tokens, model, vector_size=100):\n",
    "   \"\"\"Convert a document (list of tokens) to a vector using word embeddings.\"\"\"\n",
    "   # Initialize an empty vector\n",
    "   doc_vector = np.zeros(vector_size)\n",
    "  \n",
    "   # Count valid tokens\n",
    "   valid_token_count = 0\n",
    "  \n",
    "   # Sum up vectors for each token\n",
    "   for token in tokens:\n",
    "       if token in model.wv:\n",
    "           doc_vector += model.wv[token]\n",
    "           valid_token_count += 1\n",
    "  \n",
    "   # Average the vectors\n",
    "   if valid_token_count > 0:\n",
    "       doc_vector /= valid_token_count\n",
    "      \n",
    "   return doc_vector\n",
    "\n",
    "\n",
    "def document_to_vector_pretrained(tokens, model, vector_size=300):\n",
    "   \"\"\"Convert a document (list of tokens) to a vector using word embeddings.\"\"\"\n",
    "   # Initialize an empty vector\n",
    "   doc_vector = np.zeros(vector_size)\n",
    "  \n",
    "   # Count valid tokens\n",
    "   valid_token_count = 0\n",
    "  \n",
    "   # Sum up vectors for each token\n",
    "   for token in tokens:\n",
    "       if token in model:\n",
    "           doc_vector += model[token]\n",
    "           valid_token_count += 1\n",
    "  \n",
    "   # Average the vectors\n",
    "   if valid_token_count > 0:\n",
    "       doc_vector /= valid_token_count\n",
    "      \n",
    "   return doc_vector\n",
    "\n",
    "\n",
    "# Create document vectors for training and test sets\n",
    "X_train_w2v = np.array([document_to_vector(tokens, w2v_model) for tokens in X_train_tokens])\n",
    "X_test_w2v = np.array([document_to_vector(tokens, w2v_model) for tokens in X_test_tokens])\n",
    "\n",
    "\n",
    "print(f\"Word2Vec document vectors shape: {X_train_w2v.shape}\")\n",
    "\n",
    "\n",
    "# Alternatively, download and use pre-trained embeddings\n",
    "# This takes more time but might give better results\n",
    "try:\n",
    "   # Attempt to download pre-trained embeddings (if internet is available)\n",
    "   pretrained_model = api.load('word2vec-google-news-300')\n",
    "   print(\"Pre-trained model loaded successfully.\")\n",
    "  \n",
    "   # Create vectors using pre-trained embeddings\n",
    "   X_train_pretrained = np.array([document_to_vector_pretrained(tokens, pretrained_model, 300)\n",
    "                                   for tokens in X_train_tokens])\n",
    "   X_test_pretrained = np.array([document_to_vector_pretrained(tokens, pretrained_model, 300)\n",
    "                                  for tokens in X_test_tokens])\n",
    "  \n",
    "   print(f\"Pre-trained document vectors shape: {X_train_pretrained.shape}\")\n",
    "   pretrained_available = True\n",
    "except Exception as e:\n",
    "   print(f\"Pre-trained embeddings could not be loaded: {e}\")\n",
    "   pretrained_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b06edef",
   "metadata": {},
   "source": [
    "Step 4: Model Selectxion and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10139389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes models with different feature representations\n",
    "\n",
    "# Train with Bag of Words features\n",
    "nb_bow = MultinomialNB(alpha=1.0)\n",
    "nb_bow.fit(X_train_counts, y_train)\n",
    "\n",
    "\n",
    "# Train with TF-IDF features\n",
    "nb_tfidf = MultinomialNB(alpha=1.0)\n",
    "nb_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "\n",
    "# For Word2Vec features, we need to convert the data to a non-negative representation\n",
    "# because Multinomial NB requires non-negative feature values\n",
    "# One simple approach is min-max scaling\n",
    "def min_max_scale(X):\n",
    "   \"\"\"Scale features to [0, 1] range.\"\"\"\n",
    "   X_min = X.min(axis=0)\n",
    "   X_max = X.max(axis=0)\n",
    "   return (X - X_min) / (X_max - X_min + 1e-10)  # Adding a small epsilon to avoid division by zero\n",
    "\n",
    "\n",
    "X_train_w2v_scaled = min_max_scale(X_train_w2v)\n",
    "X_test_w2v_scaled = min_max_scale(X_test_w2v)\n",
    "\n",
    "\n",
    "# Train with Word2Vec features\n",
    "nb_w2v = MultinomialNB(alpha=1.0)\n",
    "nb_w2v.fit(X_train_w2v_scaled, y_train)\n",
    "\n",
    "\n",
    "# Also train with pre-trained embeddings if available\n",
    "if pretrained_available:\n",
    "   X_train_pretrained_scaled = min_max_scale(X_train_pretrained)\n",
    "   X_test_pretrained_scaled = min_max_scale(X_test_pretrained)\n",
    "  \n",
    "   nb_pretrained = MultinomialNB(alpha=1.0)\n",
    "   nb_pretrained.fit(X_train_pretrained_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cbfba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display top features by class\n",
    "def display_top_features(classifier, vectorizer, class_labels, n=10):\n",
    "   \"\"\"Display the top n words for each class based on their likelihood.\"\"\"\n",
    "   feature_names = vectorizer.get_feature_names_out()\n",
    "   for i, class_label in enumerate(class_labels):\n",
    "       top_indices = np.argsort(classifier.feature_log_prob_[i])[-n:][::-1]\n",
    "       top_features = [feature_names[j] for j in top_indices]\n",
    "       print(f\"\\nTop words for '{class_label}':\")\n",
    "       print(\", \".join(top_features))\n",
    "\n",
    "\n",
    "# Display top features for Bag of Words model\n",
    "print(\"\\nTop discriminative features per department (Bag of Words model):\")\n",
    "display_top_features(nb_bow, count_vectorizer, nb_bow.classes_)\n",
    "\n",
    "\n",
    "# Display top features for BoW model, focusing on bigrams\n",
    "print(\"\\nTop discriminative bigrams per department (Bag of Words model):\")\n",
    "def display_top_ngrams(classifier, vectorizer, class_labels, n=5):\n",
    "   \"\"\"Display the top n bigrams for each class based on their likelihood.\"\"\"\n",
    "   feature_names = vectorizer.get_feature_names_out()\n",
    "   # Get indices of all bigram features\n",
    "   bigram_indices = [i for i, feat in enumerate(feature_names) if ' ' in feat]\n",
    "  \n",
    "   for i, class_label in enumerate(class_labels):\n",
    "       # Filter to only consider bigram features\n",
    "       bigram_log_probs = [(j, classifier.feature_log_prob_[i][j]) for j in bigram_indices]\n",
    "       # Sort by probability (highest first)\n",
    "       top_bigram_indices = sorted(bigram_log_probs, key=lambda x: x[1], reverse=True)[:n]\n",
    "       top_bigrams = [feature_names[j] for j, _ in top_bigram_indices]\n",
    "       print(f\"\\nTop bigrams for '{class_label}':\")\n",
    "       print(\", \".join(top_bigrams))\n",
    "\n",
    "\n",
    "display_top_bigrams(nb_bow, count_vectorizer, nb_bow.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9443d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word embeddings to understand semantic relationships\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Function to visualize word embeddings in 2D\n",
    "def visualize_embeddings(model, words=None, n=50):\n",
    "   \"\"\"Visualize word embeddings in a 2D space.\"\"\"\n",
    "   # If no specific words provided, use most frequent words\n",
    "   if words is None:\n",
    "       # Get word counts from the corpus\n",
    "       from collections import Counter\n",
    "       all_words = [word for doc in df['tokenized_text'] for word in doc]\n",
    "       word_counts = Counter(all_words)\n",
    "       words = [word for word, count in word_counts.most_common(n)]\n",
    "  \n",
    "   # Filter for words in the model's vocabulary\n",
    "   valid_words = [word for word in words if word in model]\n",
    "  \n",
    "   if len(valid_words) == 0:\n",
    "       print(\"No valid words found in model vocabulary!\")\n",
    "       return\n",
    "  \n",
    "   # Get word vectors\n",
    "   word_vectors = [model[word] for word in valid_words]\n",
    "  \n",
    "   # Reduce to 2 dimensions with PCA\n",
    "   pca = PCA(n_components=2)\n",
    "   reduced_vectors = pca.fit_transform(word_vectors)\n",
    "  \n",
    "   # Plot the words in 2D space\n",
    "   plt.figure(figsize=(12, 10))\n",
    "   plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], s=10)\n",
    "  \n",
    "   # Add word labels\n",
    "   for i, word in enumerate(valid_words):\n",
    "       plt.annotate(word, (reduced_vectors[i, 0], reduced_vectors[i, 1]),\n",
    "                   fontsize=9, alpha=0.7)\n",
    "  \n",
    "   plt.title('Word Embeddings Visualization')\n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "\n",
    "\n",
    "# Visualize embeddings for department-specific words\n",
    "department_words = {\n",
    "   'technical': ['login', 'password', 'error', 'crash', 'app', 'loading', 'website'],\n",
    "   'account': ['update', 'change', 'information', 'address', 'settings', 'preferences'],\n",
    "   'billing': ['payment', 'charge', 'refund', 'credit', 'invoice', 'subscription']\n",
    "}\n",
    "\n",
    "\n",
    "# Flatten the word list\n",
    "all_dept_words = [word for words in department_words.values() for word in words]\n",
    "\n",
    "\n",
    "# Visualize the embeddings\n",
    "print(\"Visualizing word embeddings for department-specific terms:\")\n",
    "visualize_embeddings(pretrained_model, all_dept_words)\n",
    "\n",
    "\n",
    "# Also visualize some general support terms to see relationships\n",
    "general_terms = ['help', 'issue', 'problem', 'support', 'request', 'question',\n",
    "               'account', 'order', 'customer', 'service']\n",
    "visualize_embeddings(pretrained_model, general_terms + all_dept_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05dcb19",
   "metadata": {},
   "source": [
    "Step 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ce83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data for each feature representation and make predictions\n",
    "\n",
    "# Transform test data using the fitted vectorizers\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "# Word2Vec test data was already prepared above\n",
    "\n",
    "\n",
    "# Predict with Bag of Words model\n",
    "y_pred_bow = nb_bow.predict(X_test_counts)\n",
    "\n",
    "\n",
    "# Predict with TF-IDF model\n",
    "y_pred_tfidf = nb_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "\n",
    "# Predict with Word2Vec model\n",
    "y_pred_w2v = nb_w2v.predict(X_test_w2v_scaled)\n",
    "\n",
    "\n",
    "# Predict with pre-trained embeddings if available\n",
    "y_pred_pretrained = nb_pretrained.predict(X_test_pretrained_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fb20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and compare model performance\n",
    "\n",
    "# Function to evaluate and display model performance\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "   \"\"\"Evaluate a model and print performance metrics.\"\"\"\n",
    "   accuracy = accuracy_score(y_true, y_pred)\n",
    "   print(f\"\\n{model_name} Model Performance:\")\n",
    "   print(f\"Accuracy: {accuracy:.4f}\")\n",
    "   print(\"\\nClassification Report:\")\n",
    "   print(classification_report(y_true, y_pred))\n",
    "  \n",
    "   # Create a confusion matrix\n",
    "   cm = confusion_matrix(y_true, y_pred)\n",
    "  \n",
    "   # Plot confusion matrix as a heatmap\n",
    "   plt.figure(figsize=(8, 6))\n",
    "   sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "              xticklabels=sorted(set(y_true)),\n",
    "              yticklabels=sorted(set(y_true)))\n",
    "   plt.title(f'Confusion Matrix - {model_name}')\n",
    "   plt.xlabel('Predicted')\n",
    "   plt.ylabel('True')\n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "  \n",
    "   return accuracy\n",
    "\n",
    "\n",
    "# Evaluate all models\n",
    "bow_accuracy = evaluate_model(y_test, y_pred_bow, \"Bag of Words\")\n",
    "tfidf_accuracy = evaluate_model(y_test, y_pred_tfidf, \"TF-IDF\")\n",
    "w2v_accuracy = evaluate_model(y_test, y_pred_w2v, \"Word2Vec\")\n",
    "\n",
    "\n",
    "if pretrained_available:\n",
    "   pretrained_accuracy = evaluate_model(y_test, y_pred_pretrained, \"Pre-trained Embeddings\")\n",
    "\n",
    "\n",
    "# Compare models\n",
    "models = [\"Bag of Words\", \"TF-IDF\", \"Word2Vec\"]\n",
    "accuracies = [bow_accuracy, tfidf_accuracy, w2v_accuracy]\n",
    "\n",
    "\n",
    "if pretrained_available:\n",
    "   models.append(\"Pre-trained Embeddings\")\n",
    "   accuracies.append(pretrained_accuracy)\n",
    "\n",
    "\n",
    "# Plot model comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=models, y=accuracies)\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "for i, acc in enumerate(accuracies):\n",
    "   plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e6d92a",
   "metadata": {},
   "source": [
    "Step 6: Model Refinement and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eaf605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with hyperparameter tuning to improve model performance. We will go ahead and attempt to tune the TF-IDF vectorizer and associated mode\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Create a pipeline for the TF-IDF approach\n",
    "tfidf_pipeline = Pipeline([\n",
    "   ('vectorizer', TfidfVectorizer(preprocessor=advanced_preprocess, lowercase=False)),\n",
    "   ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "\n",
    "# Define parameter grid to search\n",
    "param_grid = {\n",
    "   'vectorizer__min_df': [1, 2, 3],\n",
    "   'vectorizer__max_df': [0.9, 0.95, 1.0],\n",
    "   'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "   'classifier__alpha': [0.1, 0.5, 1.0, 2.0]\n",
    "}\n",
    "\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "   tfidf_pipeline,\n",
    "   param_grid,\n",
    "   cv=5,             # 5-fold cross-validation\n",
    "   scoring='accuracy',\n",
    "   verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the grid search to the data\n",
    "print(\"Performing grid search to optimize model hyperparameters...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Print best parameters and score\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "\n",
    "# Evaluate the optimized model on test data\n",
    "optimized_pipeline = grid_search.best_estimator_\n",
    "y_pred_optimized = optimized_pipeline.predict(X_test)\n",
    "optimized_accuracy = evaluate_model(y_test, y_pred_optimized, \"Optimized TF-IDF\")\n",
    "\n",
    "\n",
    "# Add to our model comparison\n",
    "models.append(\"Optimized TF-IDF\")\n",
    "accuracies.append(optimized_accuracy)\n",
    "\n",
    "\n",
    "# Updated plot for model comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=models, y=accuracies)\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "for i, acc in enumerate(accuracies):\n",
    "   plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-env)",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
