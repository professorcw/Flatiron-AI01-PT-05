{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a38a06",
   "metadata": {},
   "source": [
    "Step 1: Loading and Exploring the California Housing Dataset\n",
    "\n",
    "Load the California Housing dataset, which contains information about various attributes of houses in California districts and their median prices. Understanding our data is crucial before building any model, so I'm examining:\n",
    "\n",
    "- The dataset structure: 20,640 samples with 8 features\n",
    "- The feature names and what they represent (MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude)\n",
    "- Basic descriptive statistics to understand the ranges and distributions\n",
    "- The distribution of our target variable (housing prices, measured in $100,000s)\n",
    "- Correlations between features and prices\n",
    "- Visual relationships between key features and housing prices\n",
    "\n",
    "The California Housing dataset is excellent for our neural network demonstration because:\n",
    "- It's a regression problem (predicting continuous housing prices).\n",
    "- It has multiple features with different scales and relationships.\n",
    "- It has over 20,000 samples, providing enough data for the network to learn meaningful patterns.\n",
    "- It contains geographical information (latitude and longitude), which introduces interesting spatial relationships.\n",
    "\n",
    "Looking at the correlations, we can see that median income (MedInc) has the strongest positive correlation with housing prices. This makes intuitive sense - areas with higher incomes tend to have more expensive housing. The scatter plots help visualize these relationships, showing both linear and non-linear patterns that our neural network will need to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81844b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Set seeda for reproducible results\n",
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "# Load the California Housing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Examine the dataset\n",
    "print(f\"California Housing dataset shape: {X.shape}\")\n",
    "print(f\"Features: {housing.feature_names}\")\n",
    "print(f\"Target variable: Median house value in $100,000s\")\n",
    "\n",
    "# View descriptive statistics\n",
    "housing_df = pd.DataFrame(X, columns=housing.feature_names)\n",
    "housing_df['PRICE'] = y\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(housing_df.describe())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Plot the distribution of housing prices\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(y, bins=30)\n",
    "plt.xlabel('Price ($100,000s)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Housing Prices')\n",
    "plt.show()\n",
    "\n",
    "# Look at correlations with target variable\n",
    "correlations = housing_df.corr()['PRICE'].sort_values(ascending=False)\n",
    "print(\"\\nFeature correlations with price:\")\n",
    "print(correlations)\n",
    "\n",
    "# Plot a few key features against price\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(housing_df['MedInc'], housing_df['PRICE'], alpha=0.5)\n",
    "plt.xlabel('Median Income (MedInc)')\n",
    "plt.ylabel('Price ($100,000s)')\n",
    "plt.title('Price vs. Median Income')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(housing_df['AveRooms'], housing_df['PRICE'], alpha=0.5)\n",
    "plt.xlabel('Average Rooms (AveRooms)')\n",
    "plt.ylabel('Price ($100,000s)')\n",
    "plt.title('Price vs. Average Rooms')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(housing_df['AveBedrms'], housing_df['PRICE'], alpha=0.5)\n",
    "plt.xlabel('Average Bedrooms (AveBedrms)')\n",
    "plt.ylabel('Price ($100,000s)')\n",
    "plt.title('Price vs. Average Bedrooms')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(housing_df['Population'], housing_df['PRICE'], alpha=0.5)\n",
    "plt.xlabel('Population')\n",
    "plt.ylabel('Price ($100,000s)')\n",
    "plt.title('Price vs. Population')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abb5765",
   "metadata": {},
   "source": [
    "Step 2: Building a Simple Neural Network\n",
    "\n",
    "Build a simple feedforward neural network with the following structure:\n",
    "- Input Layer: Accepts 8 input features from the California Housing dataset\n",
    "-  First Hidden Layer: 64 neurons with ReLU activation\n",
    "-Second Hidden Layer: 32 neurons with ReLU activation\n",
    "- Output Layer: A single neuron with no activation function (linear output)\n",
    "\n",
    "Let's break down the key architectural decisions:\n",
    "- Layer Sizes: I chose 64 neurons for the first layer and 32 for the second. This decreasing width pattern helps the network gradually distill the 8 input features into more abstract representations. The first layer is wider to capture various feature interactions, while subsequent layers consolidate this information.\n",
    "- Activation Functions: ReLU (Rectified Linear Unit) activations introduce non-linearity, allowing the network to learn complex relationships. For each neuron, ReLU outputs the input directly if it's positive, otherwise it outputs zero. This non-linearity is crucial - without it, multiple layers would simply collapse into a single linear transformation.\n",
    "- Output Layer: For regression problems like housing price prediction, we use a single output neuron with no activation function. This allows the network to predict any numerical value along the real number line, which is necessary for price predictions.\n",
    "\n",
    "The model summary reveals the parameter count for each layer:\n",
    "- Dense(64): (8 inputs × 64 outputs) + 64 biases = 576 parameters\n",
    "- Dense(32): (64 inputs × 32 outputs) + 32 biases = 2,080 parameters\n",
    "- Dense(1): (32 inputs × 1 output) + 1 bias = 33 parameters\n",
    "\n",
    "In total, this relatively simple network has 2,689 trainable parameters/weights, despite us only having access to 8 features. Each of these parameters will be adjusted during training through backpropagation and gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bf4e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network model\n",
    "model = keras.Sequential([\n",
    "    # Input layer - explicit definition for clarity\n",
    "    keras.layers.Input(shape=(8,)),  # 8 features in the California dataset\n",
    "    \n",
    "    # First hidden layer\n",
    "    keras.layers.Dense(units=64, activation='relu'),\n",
    "    \n",
    "    # Second hidden layer\n",
    "    keras.layers.Dense(units=32, activation='relu'),\n",
    "    \n",
    "    # Output layer - single neuron with no activation for regression\n",
    "    keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9797aee",
   "metadata": {},
   "source": [
    "Step 3: Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c55ea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with basic settings\n",
    "model.compile(\n",
    "    optimizer=\"RMSprop\",\n",
    "    loss='mean_squared_error',  # Standard loss for regression\n",
    "    metrics=['mae']  # Mean Absolute Error in $100,000s\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d9d9b9",
   "metadata": {},
   "source": [
    "Step 4: Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd471ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with detailed monitoring\n",
    "history = model.fit(\n",
    "    X_train,                  # Input features\n",
    "    y_train,                  # Target housing prices\n",
    "    batch_size=64,            # Process 64 examples per gradient update\n",
    "    epochs=100,               # Maximum number of passes through the dataset\n",
    "    validation_split=0.2,     # Use 20% of training data for validation\n",
    "    verbose=1                 # Show progress during training\n",
    ")\n",
    "\n",
    "# Store training metrics for analysis\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_mae = history.history['mae']\n",
    "val_mae = history.history['val_mae']\n",
    "epochs_range = range(1, len(train_loss) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590b86b",
   "metadata": {},
   "source": [
    "Step 5: Analyzing Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b0e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_loss, label='Training Loss (MSE)')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Loss During Training')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Plot mean absolute error\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, train_mae, label='Training MAE')\n",
    "plt.plot(epochs_range, val_mae, label='Validation MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error ($100,000s)')\n",
    "plt.title('Mean Absolute Error During Training')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final training stats\n",
    "print(f\"Final training loss (MSE): {train_loss[-1]:.4f}\")\n",
    "print(f\"Final validation loss (MSE): {val_loss[-1]:.4f}\")\n",
    "print(f\"Final training MAE: ${train_mae[-1]:.4f} ($100,000s)\")\n",
    "print(f\"Final validation MAE: ${val_mae[-1]:.4f} ($100,000s)\")\n",
    "\n",
    "# Check if training was successful\n",
    "if val_loss[-1] < val_loss[0]:\n",
    "    improvement = (1 - val_loss[-1]/val_loss[0]) * 100\n",
    "    print(f\"Model improved by {improvement:.1f}% during training.\")\n",
    "else:\n",
    "    print(\"Model did not improve during training. Consider adjusting hyperparameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a164771",
   "metadata": {},
   "source": [
    "Step 6: Evaluating the Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss (MSE): {test_loss:.4f}\")\n",
    "print(f\"Test MAE: ${test_mae:.4f} ($100,000s)\")\n",
    "\n",
    "# Make predictions and analyze errors\n",
    "predictions = model.predict(X_test)\n",
    "errors = predictions.flatten() - y_test\n",
    "\n",
    "# Calculate key error metrics\n",
    "mean_error = np.mean(errors)\n",
    "median_error = np.median(errors)\n",
    "max_error = np.max(np.abs(errors))\n",
    "\n",
    "print(f\"Mean prediction error: ${mean_error:.4f} ($100,000s)\")\n",
    "print(f\"Median prediction error: ${median_error:.4f} ($100,000s)\")\n",
    "print(f\"Maximum prediction error: ${max_error:.4f} ($100,000s)\")\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, predictions, alpha=0.5)\n",
    "plt.plot([0, 5], [0, 5], 'r--')  # Perfect prediction line\n",
    "plt.xlabel('Actual Price ($100,000s)')\n",
    "plt.ylabel('Predicted Price ($100,000s)')\n",
    "plt.title('Actual vs. Predicted Housing Prices')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(errors, bins=50)\n",
    "plt.xlabel('Prediction Error ($100,000s)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Prediction Errors')\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Check error by price range\n",
    "price_ranges = [0, 1, 2, 3, 4, 5]\n",
    "for i in range(len(price_ranges)-1):\n",
    "    # Filter test data for this price range\n",
    "    mask = (y_test >= price_ranges[i]) & (y_test < price_ranges[i+1])\n",
    "    range_mae = np.mean(np.abs(errors[mask]))\n",
    "    range_count = np.sum(mask)\n",
    "    \n",
    "    if range_count > 0:\n",
    "        print(f\"MAE for houses ${price_ranges[i]}-{price_ranges[i+1]} million: ${range_mae:.4f} ($100,000s), {range_count} houses\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
